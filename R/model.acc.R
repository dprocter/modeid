####### TODO
# add F1, and other model accuracy scores
#link details to wiki page describing accuracy measures

#' @title Model accuracy
#' @description
#' Returns model accuracy from a confusion matrix derived from
#' \code{\link{confusion.matrix}}
#' @param conf.mat a confusion matrix generated by \code{\link{confusion.matrix}}
#' @return A three column data.frame, with the precision and recall per mode
#' @details
#' Return precision and recall for each mode.
#'
#' Precision: the percentage of points which the model predicts as each mode
#' which are also observed as that mode
#'
#' Recall: the percentage of points which are observed as each mode that the model
#' correctly predicts
#'
#'  @examples
#' pred<-numeric(10)+1
#' pred[5:6]<-2
#' pred<-factor(pred,labels=c("Mode1","Mode2"))
#'
#' obs<-numeric(10)+1
#' obs[6:8]<-2
#' obs<-factor(obs,labels=c("Mode1","Mode2"))
#'
#' model.acc(confusion.matrix(pred,obs))

model.acc<-function(conf.mat){
  no.modes<-length(conf.mat[,1])

  precision<-numeric(no.modes)
  recall<-numeric(no.modes)
  modes<-rownames(conf.mat)

  for (i in 1:no.modes){
    precision[i]<-conf.mat[i,i]/sum(conf.mat[i,])*100
    recall[i]<-conf.mat[i,i]/sum(conf.mat[,i])*100
  }
  return(data.frame(modes,precision,recall))
}
