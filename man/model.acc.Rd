% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/model.acc.R
\name{model.acc}
\alias{model.acc}
\title{Model accuracy}
\usage{
model.acc(conf.mat)
}
\arguments{
\item{conf.mat}{a confusion matrix generated by \code{\link{confusion.matrix}}}
}
\value{
A seven column data.frame, with accuracy scores per mode
}
\description{
Returns model accuracy from a confusion matrix derived from
\code{\link{confusion.matrix}}
}
\details{
Return accuracy scores for each mode, all expressed as percentages.

PPV, Positive predictive value (aka Precision): the percentage of points which the model predicts as each mode
which are also observed as that mode

Sensitivity (aka Recall, Detection rate, True Posistive rate): the ratio of true positives
to true positives and false negatives

NPV: Negative predictive value: The ratio of true negatives and false negatives

Specificity (aka True Neagtive Rate): The ratio of true negatives to true negatives and false positives

Accuracy: The sum of true positives and negatives divided by the sum of true and false negatives and positives

F1 score: The harmonic mean of PPV and sensitivity

 @examples
pred<-numeric(10)+1
pred[5:6]<-2
pred<-factor(pred,labels=c("Mode1","Mode2"))

obs<-numeric(10)+1
obs[6:8]<-2
obs<-factor(obs,labels=c("Mode1","Mode2"))

model.acc(confusion.matrix(pred,obs))
}
