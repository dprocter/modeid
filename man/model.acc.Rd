% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/model.acc.R
\name{model.acc}
\alias{model.acc}
\title{Model accuracy}
\usage{
model.acc(conf.mat)
}
\arguments{
\item{conf.mat}{a confusion matrix generated by \code{\link{confusion.matrix}}}
}
\value{
A seven column data.frame, with accuracy scores per mode
}
\description{
Returns model accuracy from a confusion matrix derived from
\code{\link{confusion.matrix}}
}
\details{
Return accuracy scores for each mode, all expressed as percentages. 
There are many names for different accuracy scores, and some scores are preferential in certain
 situations, see https://en.wikipedia.org/wiki/Sensitivity_and_specificity

Scores output:

1. PPV, Positive predictive value (aka Precision): the percentage of points which the model predicts as each mode
which are also observed as that mode

2. Sensitivity (aka Recall, Detection rate, True Positive rate): the ratio of true positives
to true positives and false negatives

3. NPV: Negative predictive value: The ratio of true negatives and false negatives

4. Specificity (aka True Negative Rate): The ratio of true negatives to true negatives and false positives

5. Accuracy: The sum of true positives and negatives divided by the sum of true and false negatives and positives

6. F1 score: The harmonic mean of PPV and sensitivity
}
\examples{
pred<-numeric(10)+1
pred[5:6]<-2
pred<-factor(pred,labels=c("Mode1","Mode2"))

obs<-numeric(10)+1
obs[6:8]<-2
obs<-factor(obs,labels=c("Mode1","Mode2"))

model.acc(confusion.matrix(pred,obs))
}
