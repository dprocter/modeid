---
title: "modeid usage example"
author: "Duncan Procter"
date: "8 September 2017"
output: html_document
---


```{r setup, include=FALSE}

accfile<-"C:/Duncan/ENABLE/BST/Example data/Acc/bob10sec.csv"
gpsfile<-"C:/Duncan/ENABLE/BST/Example data/GPS/bob.csv"

```

## Requirements
To use this example you will need quite a few packages installed. Going though it you can see where each is loaded. The modeid package is required throughout, and can be installed from github via a devtools function, install.github:

``` {r libraries, eval=FALSE}
install.packages("devtools","zoo","randomForest","sp","rgdal","maptools","spatstat")
library(devtools)
install.githhub("dprocter/modeid",quiet=TRUE)
```


## Training data

How you extract your training data-set based on our rules will depend on how your demgraphic data is set up, providing an example of how we did it is not particularly useful. if you need help feel free to contact the author.

We have privided an anonymised version of our training dataset, so you can see how we use it:

```{r training data}
library(modeid)
training.data<-Proc2017tdata
summary(training.data)

```

### Replicating our cross.validation

The function *cross.validator* fits a number of random forest models to the subsets of the data you specify.

As you can see below you must first provide a dataset free from NAs, so we first kick out unused variables and then use na.omit to remove NAs

*cross.validator* then requires the NA free dataste, the formula of the model you wish to repeatedly fit, and a marker to denote the cross-validation subsets. We simply randomly assigned each participant to one of 5 subsets, see ?sample for random number selection

*cross.validator* currently has horrible looking output, which consists of 4 columns (the fitted models, the test data.frames, the confusion matrices and model accuracy scores). The rows correspond to the cross-validation subsets, so in our example there will be 5. This will be tidied in future versions. 

We use set.seed to give the randomness a start point, but we cannot account for all of it in the cross-validation. Therefor, the output will slightly disagree with our reported scores. It also needs to be linked back to the original training data to include points we couldn't predict to (variables contained NAs), as errors, though these are tiny in number (under 100 points)


```{r crossvalidation}

train.for.cv<-subset(training.data,select=-c(Axis1,Axis2,Axis3,speed,pdop,hdop,vdop,sumsnr,near.train
                                             ,dist.next.min,dist.last.min))
train.for.cv<-na.omit(train.for.cv)

set.seed(315)
cv.model<-cross.validator(training.dataset = train.for.cv
                ,formula = true.mode~ax1.mean+ax1.sd+ax1.cent.90+ax1.cent.10
                         +spd.mean+spd.sd+spd.cent.90+spd.cent.10
                         +sumsnr.mean+near.train.4min+dist.4min.nextmin+dist.4min.lastmin
                ,cv.marker = train.for.cv$cv.marker)

#This is ugly, but gives an overall confusion matrix 
cv.overall<-cv.model[1,3][[1]]+cv.model[2,3][[1]]+cv.model[3,3][[1]]+cv.model[4,3][[1]]+cv.model[5,3][[1]]
cv.overall

model.acc(cv.overall)

```


## Data Processing

Let us assume:

1. You have Accelerometer files which have been exported to whatever epoch length you used e.g. 10 seconds.

2. You have corresponding GPS data

### Merging accelerometer and GPS data

First we need to merge the data together. The following takes accfile, and gpsfile, which I have assigned to the appropriate file paths. 

Cutoff.method = 3 means that we will cut the first day of data, then take 7 days of data, see ?gps.acc.merged for more details

British.time=1 means that the data is from the UK, so we need to convert UTC time from the GPS unit to British Summer time, during the appropriate months, so it matches the correct accelerometer data.

```{r merging}
merged.data<-gps.acc.merge(accfile = accfile
                          ,gpsfile = gpsfile
                          ,participant.id = "bob"
                          ,cutoff.method = 3
                          ,epoch.length = 10
                          ,british.time = 1
                          ,acc.model = "Actigraph")

summary(merged.data)
```

### Removing accelerometer non-wear time

Accelerometer non-wear time will be identified as periods of 0 counts on all accelerometer axes.

window.length specifies the width of the window in minutes

interruption.length allows you to specify any allowed interruptions (epochs with over 0 counts), again in minutes

The following code therefore identifies non-wear time in the dataset merged.data, where you require 1 hour of 0 counts to denote non-wear, and we allow 2 minutes of interruptions.

All accelerometer axis data is set as NA where it seems to be non-wear

```{r non-wear}
merged.data<-acc.nonwear(dataset= merged.data
                           ,epoch.length = 10
                           ,window.length = 60
                           ,interruption.length = 2)
summary(merged.data)
```

### Cleaning GPS data

There are sevral circumstances is which GPS data can be unreliable (usually caused by poor signal). Therefore we remove points we do not think we can trust.

We clean the data in 3 ways:

1. Using a speed cut-off, to remove implausibly high speed points

2. Using a Horisontal Dilution of Precision cut-off, to remove points where the satellites are aligned and so signal is poor

3. By removing points that are isolated, and thefore have no context

The following therefore marks all GPS data where speed is under 160kph, hdop is under 5, or there are less than 3 points within 5 minutes (2.5 minutes before and after the points, inluding the point itself, therefore 2 neighbours).

The data.loss.gps function tell you how many points are removed at each level of processing

```{r gps clean}
data.loss.gps(speed.cutoff = 160
              ,hdop.cutoff = 5
              ,neighbour.number = 3
              ,neighbour.window = 300
              ,epoch.length = 10
              ,dataset = merged.data)

merged.data<-gps.cleaner(speed.cutoff = 160
                         ,hdop.cutoff = 5
                         ,neighbour.number = 3
                         ,neighbour.window = 300
                         ,epoch.length = 10
                         ,dataset = merged.data)

```

### Calculating distance to train lines

This doesn't cover getting the neccessary data on train lines. As a start point, if you're in a UK institution there is lots of data freely available on Digimap. 

We use the sp and rgdal packages to import an ArcGIS shapefile into R of train lines, then convert the SpatialLinesDataFrame into a psp so we can use the spatstat function nncross to measure distance from each point to the nearest train line. For the participants data we make SpatialPointsDataFrame, then reproject it to OSGB1936, used by the train data, and then convert it into a ppp to allow us to use the spatstat function.


``` {r train lines}
library(sp)
library(maptools)
library(rgdal)
library(spatstat)

train.lines<-readOGR("C:/Duncan/Train lines","all_train_lines")

train.coords<-coordinates(train.lines)
max.x<-max(unlist(lapply(train.coords,FUN=function(x){x[[1]][,1]})))
max.y<-max(unlist(lapply(train.coords,FUN=function(x){x[[1]][,2]})))
min.x<-min(unlist(lapply(train.coords,FUN=function(x){x[[1]][,1]})))
min.y<-min(unlist(lapply(train.coords,FUN=function(x){x[[1]][,2]})))

train.win<-owin(xrange=c(min.x,max.x),yrange=c(min.y,max.y))

train.psp<-as.psp(train.lines,W=train.win)

 # add the near train data to the merged dataset
  merged.data$near.train<-NA

  # take a subset of the data that has valid GPS data
  # and turn it into a SpatialPointsDataFrame, with projection information
  only.gps<-subset(merged.data,!is.na(speed))
  merged.data$easting<-NA
  merged.data$northing<-NA
  if (length(only.gps[,1])>0){
    gps.spatial<-SpatialPointsDataFrame(cbind(only.gps$longitude,only.gps$latitude)
                                        ,data=only.gps,proj4string = CRS("+proj=longlat +datum=WGS84"))


    # convert gps.spatial to have the same projection as the train.lines data
    gps.spatial<-spTransform(gps.spatial,CRS(proj4string(train.lines)))

    #creates a bounding box around the points (ppp's need these)
    gps.win<-owin(xrange=c(min(coordinates(gps.spatial)[,1]-1000),max(coordinates(gps.spatial)[,1])+1000)
                  ,yrange=c(min(coordinates(gps.spatial)[,2]-1000),max(coordinates(gps.spatial)[,2]+1000)))
    # turns the gps data into a sptial point pattern
    gps.ppp<-as.ppp(coordinates(gps.spatial),W=gps.win)


    # the nncross function from spatstat gives you sitance from each point to the nearest line
    merged.data$near.train[!is.na(merged.data$speed)]<-nncross(gps.ppp,train.psp)[,1]

    merged.data$easting[!is.na(merged.data$longitude)]<-coordinates(gps.spatial)[,1]
    merged.data$northing[!is.na(merged.data$longitude)]<-coordinates(gps.spatial)[,2]
  }

```

### Distance 1 minute away

When not travelling, people stay in one spot. To take this into consideration we include distance moved in the next minute and distance moved in the last minute. Both are included so that we can detect no movement both just as you stopped and just before you start moving too.

``` {r dist to last}

merged.data$dist.next.min<-distance.moved(dataset = merged.data,
                                          last=FALSE,
                                          time.window = 60,
                                          epoch.length = 10)

merged.data$dist.last.min<-distance.moved(dataset = merged.data,
                                          last=TRUE,
                                          time.window = 60,
                                          epoch.length = 10)
summary(merged.data)


```


### Calculating moving windows

None of the previous functions remove invalid data from the dataset, they only set the relevant accelerometer or GPS variables as NA when we have reason to think they are untrustworthy. As a result the dataset is a continuous set of epochs from the start to end of the data. We can therefore treat a moving window across the cleaned merged dataset as a time window, as long as we allow for how long each epoch represents.

To calculate moving windows we use the zoo package, and particularly the *rollapply* function. Rollapply allows us to specify a function and then apply it across a window. We use a width of four minutes, centered on the point of interest, so here that is 25 epochs (12 before, 12 after, each 10 seconds plus the center point). All the functions are simple mean, sd or quantile, but with na.omit, so they ignore small numbers of NAs in the window. partial=TRUE means that if not all pointsjion ;/ in the 25 are there (e.g. it is the start of the dataset), the function is still applied to the remaining points. 

```{r moving windows}
library(zoo)

  merged.data$ax1.mean<-rollapply(merged.data$Axis1,width=25,align="center",FUN=function(x){mean(na.omit(x))},partial=TRUE,fill=NA)
  merged.data$ax1.sd<-rollapply(merged.data$Axis1,width=25,align="center",FUN=function(x){sd(na.omit(x))},fill=NA,partial=TRUE)
  merged.data$ax1.cent.10<-rollapply(merged.data$Axis1,width=25,align="center",FUN=function(x){quantile(x,0.1,na.rm = TRUE)},fill=NA,partial=TRUE)
  merged.data$ax1.cent.90<-rollapply(merged.data$Axis1,width=25,align="center",FUN=function(x){quantile(x,0.9,na.rm = TRUE)},fill=NA,partial=TRUE)

  merged.data$spd.mean<-rollapply(merged.data$speed,width=25,align="center",FUN=function(x){mean(na.omit(x))},fill=NA,partial=TRUE)
  merged.data$spd.sd<-rollapply(merged.data$speed,width=25,align="center",FUN=function(x){sd(na.omit(x))},fill=NA,partial=TRUE)
  merged.data$spd.cent.10<-rollapply(merged.data$speed,width=25,align="center",FUN=function(x){quantile(x,0.1,na.rm = TRUE)},fill=NA,partial=TRUE)
  merged.data$spd.cent.90<-rollapply(merged.data$speed,width=25,align="center",FUN=function(x){quantile(x,0.9,na.rm = TRUE)},fill=NA,partial=TRUE)

  merged.data$sumsnr.mean<-rollapply(merged.data$sumsnr,width=25,align="center",FUN=function(x){mean(na.omit(x))},fill=NA,partial=TRUE)
  merged.data$near.train.4min<-rollapply(merged.data$near.train,width=25,align="center",FUN=function(x){mean(na.omit(x))},fill=NA,partial=TRUE)
  
    merged.data$dist.4min.lastmin<-rollapply(merged.data$dist.last.min,width=25,align="center",FUN=function(x){mean(na.omit(x))},fill=NA,partial=TRUE)
      merged.data$dist.4min.nextmin<-rollapply(merged.data$dist.next.min,width=25,align="center",FUN=function(x){mean(na.omit(x))},fill=NA,partial=TRUE)

```

### Predicting to the example data

Prediction to participants is simple once you have calculated the rolling means:

```{r prediction}
library(randomForest)

set.seed(315)
base.model<-randomForest(true.mode~ax1.mean+ax1.sd+ax1.cent.90+ax1.cent.10
                         +spd.mean+spd.sd+spd.cent.90+spd.cent.10
                         +sumsnr.mean+near.train.4min+dist.4min.lastmin+dist.4min.nextmin
                         ,data=train.for.cv,importance=TRUE)

merged.data$pred.mode<-predict(base.model,newdata = merged.data,type="response")
summary(merged.data$pred.mode)
plot(merged.data$longitude,merged.data$latitude,axes=FALSE,xlab="",ylab="",col=merged.data$pred.mode,pch=19)

```


